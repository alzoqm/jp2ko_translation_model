{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_translation_kojp_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOp8/ixZApJ8d7E0xTsg81/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alzoqm/jp2ko_translation_model/blob/main/transformer_translation_kojp_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l42tHN1U_4FV"
      },
      "source": [
        "# 모듈 및 vocab 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM7PcHzN_ykD",
        "outputId": "e1eeb81a-d460-4b21-fae7-89a661a5d9a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a7qyBoX_vcD",
        "outputId": "4df08ade-31b6-4d56-dd72-503abadc2c88"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akOmxkOdNKKv"
      },
      "source": [
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import time\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from tqdm import *"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvRPvMQK_3Xj",
        "outputId": "5521593c-dab1-4863-f291-43d058a411f8"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.87.130.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.87.130.178:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dG_lTXC7__CU"
      },
      "source": [
        "# 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yek17aVoNR8c"
      },
      "source": [
        "class PositionalEncoding(Layer):\n",
        "  def __init__(self, position, d_model):\n",
        "    super().__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(1000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model = d_model\n",
        "    )\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "    cos = tf.math.cos(angle_rads[:, 1::2])\n",
        "    angle_rads = np.zeros(angle_rads.shape)\n",
        "    angle_rads[:, 0::2] = sines\n",
        "    angle_rads[:, 1::2] = cos\n",
        "\n",
        "    pos_encoding = tf.constant(angle_rads)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "    print(pos_encoding.shape)\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OVyQU5GNUVp"
      },
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model // num_heads\n",
        "    self.depth = d_model // num_heads\n",
        "    self.scale = 1 / (self.depth ** 0.5)\n",
        "\n",
        "    self.q_linear = tf.keras.layers.Dense(d_model)\n",
        "    self.k_linear = tf.keras.layers.Dense(d_model)\n",
        "    self.v_linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, q, k, v, mask):\n",
        "    #batch_size = q.shape[0]\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.q_linear(q)\n",
        "    k = self.k_linear(k)\n",
        "    v = self.v_linear(v)\n",
        "\n",
        "    q = tf.reshape(q, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "\n",
        "    k = tf.reshape(k, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
        "\n",
        "    v = tf.reshape(v, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    logits = matmul_qk / self.scale\n",
        "\n",
        "    if mask is not None:\n",
        "      logits += (mask * -1e9)\n",
        "\n",
        "    attn_mat = tf.nn.softmax(logits, axis=-1)\n",
        "    outputs = tf.matmul(attn_mat, v)\n",
        "    outputs = tf.transpose(outputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    outputs = tf.reshape(outputs, shape=(batch_size, -1, self.d_model))\n",
        "    outputs = self.linear(outputs)\n",
        "    return outputs, attn_mat"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZpo4a0NPKji"
      },
      "source": [
        "class MLP(Layer):\n",
        "  def __init__(self, dff, d_model):\n",
        "    super().__init__()\n",
        "    self.linear1 = tf.keras.layers.Dense(dff)\n",
        "    self.linear2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    outputs = self.linear1(inputs)\n",
        "    outputs = tf.nn.gelu(outputs)\n",
        "    outputs = self.linear2(outputs)\n",
        "    return outputs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8df6JeQPpW"
      },
      "source": [
        "class EncoderLayer(tf.keras.Model):\n",
        "  def __init__(self, dff, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.attn_layer = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mlp = MLP(dff, d_model)\n",
        "\n",
        "    self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n",
        "    self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, inputs, mask):\n",
        "    attn_outputs, attn_mat = self.attn_layer(inputs, inputs, inputs, mask)\n",
        "    attn_outputs = self.dropout1(attn_outputs)\n",
        "    attn_outputs = self.layer_norm1(inputs + attn_outputs)\n",
        "\n",
        "    outputs = self.mlp(attn_outputs)\n",
        "    outputs = self.dropout2(outputs)\n",
        "    outputs = self.layer_norm2(attn_outputs + outputs)\n",
        "    return outputs, attn_mat"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZo7DUMSRSHs"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, num_layers, dff, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.word_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_embedding = PositionalEncoding(vocab_size, d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    self.encoder_layer = [EncoderLayer(dff, d_model, num_heads, dropout) for _ in range(num_layers)]\n",
        "\n",
        "  def call(self, inputs, mask):\n",
        "    attn_mats = []\n",
        "    embedding = self.word_embedding(inputs)\n",
        "    embedding = self.pos_embedding(embedding)\n",
        "    outputs = self.dropout(embedding)\n",
        "\n",
        "    for layer in self.encoder_layer:\n",
        "      outputs, attn_mat = layer(outputs, mask)\n",
        "      attn_mats.append(attn_mat)\n",
        "    \n",
        "    return outputs, attn_mats"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek3kWGX8So9P"
      },
      "source": [
        "class DecoderLayer(tf.keras.Model):\n",
        "  def __init__(self, dff, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.masked_attn_layer = MultiHeadAttention(d_model, num_heads)\n",
        "    self.norm_attn_layer = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mlp = MLP(dff, d_model)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n",
        "    self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n",
        "    self.layer_norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n",
        "\n",
        "  def call(self, dec_inputs, enc_outputs, padding_mask, look_ahead_mask):\n",
        "    mask_attn_outputs, mask_attn_mat = self.masked_attn_layer(dec_inputs, dec_inputs, dec_inputs, look_ahead_mask)\n",
        "    mask_attn_outputs = self.layer_norm1(dec_inputs + mask_attn_outputs)\n",
        "\n",
        "    norm_attn_outputs, norm_attn_mat = self.norm_attn_layer(mask_attn_outputs, enc_outputs, enc_outputs, padding_mask)\n",
        "    norm_attn_outputs = self.dropout1(norm_attn_outputs)\n",
        "    norm_attn_outputs = self.layer_norm2(mask_attn_outputs + norm_attn_outputs)\n",
        "\n",
        "    outputs = self.mlp(norm_attn_outputs)\n",
        "    outputs = self.dropout2(outputs)\n",
        "    outputs = self.layer_norm3(norm_attn_outputs + outputs)\n",
        "\n",
        "    return outputs, mask_attn_mat, norm_attn_mat"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2_7cmUhwz7X"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, num_layers, dff, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.word_embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_embedding = PositionalEncoding(vocab_size, d_model)\n",
        "\n",
        "    self.decoder_layer = [DecoderLayer(dff, d_model, num_heads, dropout) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, dec_inputs, enc_outputs, padding_mask, look_ahead_mask):\n",
        "    mask_attn_mats, norm_attn_mats = [],  []\n",
        "    embedding = self.word_embedding(dec_inputs)\n",
        "    embedding = self.pos_embedding(embedding)\n",
        "    outputs = self.dropout(embedding)\n",
        "\n",
        "    for layer in self.decoder_layer:\n",
        "      outputs, mask_attn_mat, norm_attn_mat  = layer(outputs, enc_outputs, padding_mask, look_ahead_mask)\n",
        "      mask_attn_mats.append(mask_attn_mats)\n",
        "      norm_attn_mats.append(norm_attn_mat)\n",
        "\n",
        "    return outputs, mask_attn_mats, norm_attn_mats"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39oRlf-j0rYW"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, enc_vocab_size, dec_vocab_size, num_layers, dff, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab_size, num_layers, dff, d_model, num_heads, dropout)\n",
        "    self.decoder = Decoder(dec_vocab_size, num_layers, dff, d_model, num_heads, dropout)\n",
        "    self.linear = tf.keras.layers.Dense(dec_vocab_size)\n",
        "\n",
        "  def create_padding_mask(self, inputs):\n",
        "    mask = tf.cast(tf.math.equal(inputs, 0), dtype=tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  def create_look_ahead_mask(self, inputs):\n",
        "    max_len = inputs.shape[1]\n",
        "    #max_len = tf.shape(inputs)[1]\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones(shape=(max_len, max_len)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "  def create_mask(self, enc_inputs, dec_inputs):\n",
        "    enc_padding_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "    dec_padding_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "    look_ahead_mask = self.create_look_ahead_mask(dec_inputs)\n",
        "    dec_look_padding_mask = self.create_padding_mask(dec_inputs)\n",
        "    return enc_padding_mask, dec_padding_mask, tf.maximum(look_ahead_mask, dec_look_padding_mask)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    enc_inputs, dec_inputs = inputs\n",
        "    print(enc_inputs)\n",
        "    enc_padding_mask, dec_padding_mask, look_ahead_mask = self.create_mask(enc_inputs, dec_inputs)\n",
        "    \n",
        "    enc_outputs, enc_attn_mats = self.encoder(enc_inputs, enc_padding_mask)\n",
        "    dec_outputs, mask_attn_mats, dec_attn_mats = self.decoder(dec_inputs, enc_outputs, dec_padding_mask, look_ahead_mask)\n",
        "\n",
        "    outputs = self.linear(dec_outputs)\n",
        "    return outputs#, enc_attn_mats, mask_attn_mats, dec_attn_mats"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-nLEc1YBWNA"
      },
      "source": [
        "# model parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm6NcEVrBSmk"
      },
      "source": [
        "ENC_VOCAB_SIZE = 12007\n",
        "DEC_VOCAB_SIZE = 12007\n",
        "NUM_LAYERS = 4\n",
        "D_MODEL = 768\n",
        "DFF = D_MODEL*4\n",
        "NUM_HEADS = 8\n",
        "DROPOUT = 0.1\n",
        "MAX_LEN = 350"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz9QCyV4AIz3"
      },
      "source": [
        "# data 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8pUQxMb9bng"
      },
      "source": [
        "train_path_dir = \"/content/drive/MyDrive/ColabNotebooks/project/translation/dataset/[라벨]ko2ja_culture_training_json.zip (Unzipped Files)\"\n",
        "val_path_dir = \"/content/drive/MyDrive/ColabNotebooks/project/translation/dataset/[라벨]ko2ja_culture_validation_json.zip (Unzipped Files)\"\n",
        "train_file_list = os.listdir(train_path_dir)\n",
        "val_file_list = os.listdir(val_path_dir)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PICeoVqt-FNE"
      },
      "source": [
        "train_data_df = pd.DataFrame()\n",
        "for file in train_file_list:\n",
        "  df = pd.read_json(path_or_buf=train_path_dir + '/' + file)\n",
        "  train_data_df = pd.concat([train_data_df, df], ignore_index=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mabikXpC_Flw"
      },
      "source": [
        "for file in val_file_list:\n",
        "  df = pd.read_json(path_or_buf=val_path_dir + '/' + file)\n",
        "  train_data_df = pd.concat([train_data_df, df], ignore_index=False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP7BLgnKAN9g",
        "outputId": "91c478c7-4cff-4b65-db35-f4450fb5eb71"
      },
      "source": [
        "ko_vocab_file = \"/content/drive/MyDrive/ColabNotebooks/project/translation/vocab/kowiki.model\"\n",
        "ko_tokenizer = spm.SentencePieceProcessor()\n",
        "ko_tokenizer.load(ko_vocab_file)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUeX9cmSATs-",
        "outputId": "dd756d80-d63d-499b-c711-a120274b2f41"
      },
      "source": [
        "ja_vocab_file = \"/content/drive/MyDrive/ColabNotebooks/project/translation/vocab/jpwiki.model\"\n",
        "ja_tokenizer = spm.SentencePieceProcessor()\n",
        "ja_tokenizer.load(ja_vocab_file)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yhLUQ2lA0yl"
      },
      "source": [
        "korean = []\n",
        "japanese = []\n",
        "for ko_sentence, ja_sentence in zip(train_data_df['한국어'], train_data_df['일본어']):\n",
        "  korean.append(ko_sentence)\n",
        "  japanese.append(ja_sentence)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqKBGk9IEKUO"
      },
      "source": [
        "del train_data_df"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDvkoCl1Bm3P"
      },
      "source": [
        "korean_pad_data = []\n",
        "\n",
        "for line in korean:\n",
        "  input = ko_tokenizer.encode_as_ids(line)\n",
        "  input.insert(0, 2) #begin token\n",
        "  input.append(3) #end token\n",
        "  korean_pad_data.append(input)\n",
        "korean_pad_data = tf.keras.preprocessing.sequence.pad_sequences(korean_pad_data, maxlen=MAX_LEN, padding='post')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EskpUaVLEMFx"
      },
      "source": [
        "del korean"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPneamd0Bybc"
      },
      "source": [
        "japanese_pad_data = []\n",
        "\n",
        "for line in japanese:\n",
        "  input = ja_tokenizer.encode_as_ids(line)\n",
        "  input.insert(0, 2)\n",
        "  input.append(3)\n",
        "  japanese_pad_data.append(input)\n",
        "japanese_pad_data = tf.keras.preprocessing.sequence.pad_sequences(japanese_pad_data, maxlen=MAX_LEN, padding='post')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VInGlU_zEPMo"
      },
      "source": [
        "del japanese"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uewT2HEFBz-8",
        "outputId": "fcd8064f-b4b0-46f8-8c95-29ad809238bf"
      },
      "source": [
        "print(korean_pad_data.shape)\n",
        "print(japanese_pad_data.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1350000, 350)\n",
            "(1350000, 350)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8lWb7noxTO-"
      },
      "source": [
        "BATCH_SIZE_PER_REPLICA = 4\n",
        "global_batch_size = (BATCH_SIZE_PER_REPLICA *\n",
        "                     strategy.num_replicas_in_sync)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO1A4efkx-86",
        "outputId": "3fce2437-3fa2-4d3a-a675-96363374be84"
      },
      "source": [
        "strategy.num_replicas_in_sync"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    japanese_pad_data, korean_pad_data\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "#dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(32)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "b1Hz0LtQ3c3H"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbMxQJ-PCHZr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=32"
      ],
      "metadata": {
        "id": "wlPv9Vhl5Yrn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6-X9n5YB9F-"
      },
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  model = create_model()\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(iterator):\n",
        "  \"\"\"The step function for one training step.\"\"\"\n",
        "\n",
        "  def step_fn(inputs):\n",
        "    \"\"\"The computation to run on each TPU device.\"\"\"\n",
        "    japanese, korean = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model([japanese, korean[:, :-1]], training=True)\n",
        "      loss = loss_function(korean[: 1:], logits)\n",
        "      loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
        "    training_loss.update_state(loss * strategy.num_replicas_in_sync)\n",
        "\n",
        "  strategy.run(step_fn, args=(next(iterator),))"
      ],
      "metadata": {
        "id": "oWxf-_E442IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  model = Transformer(ENC_VOCAB_SIZE, DEC_VOCAB_SIZE, NUM_LAYERS, DFF, D_MODEL, NUM_HEADS, DROPOUT)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSz9-N48-Ag0",
        "outputId": "4beb193d-41e6-4ed1-ab38-f9e770821db3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12007, 768)\n",
            "(1, 12007, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = strategy.experimental_distribute_dataset(dataset)"
      ],
      "metadata": {
        "id": "gHfOhuD9-9iI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inputs):\n",
        "  japanese, korean = inputs\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model([japanese, korean[:, :-1]], training=True)\n",
        "    loss = loss_function(korean[:, 1:], logits)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  return loss \n",
        "\n",
        "@tf.function\n",
        "def distributed_train_step(inputs):\n",
        "  per_replica_losses = strategy.run(train_step, args=(inputs,))\n",
        "  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                         axis=None)\n",
        "\n",
        "for epoch in range(10):\n",
        "  total_loss = 0.0\n",
        "  num_batch = 0\n",
        "  with tqdm(total=1350000//32 + 1, desc=f\"Train({epoch})\") as pbar:\n",
        "    for x in train_dataset:\n",
        "      total_loss += distributed_train_step(x)\n",
        "      num_batch += 1\n",
        "\n",
        "      pbar.update(1)\n",
        "  train_loss = total_loss / num_batch\n",
        "\n",
        "  model.save_weights('/content/drive/MyDrive/ColabNotebooks/project/translation/model/jp2ko_translation_model_weights.h5', overwrite=True)"
      ],
      "metadata": {
        "id": "HHn-9h2t-G8c"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96WL7q-s0s4x"
      },
      "source": [
        "START_TOKEN, END_TOKEN = [ko_tokenizer.bos_id()], [ko_tokenizer.eos_id()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate and Predict"
      ],
      "metadata": {
        "id": "2kzoxAxWDxu7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz5m64Ru1lcS"
      },
      "source": [
        "def evaluate(sentence, model):\n",
        "  if len(sentence) > MAXLEN:\n",
        "    print(\"최대 길이 초과\")\n",
        "    return\n",
        "\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + ja_tokenizer.encode_as_ids(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 예측 시작\n",
        "  for i in range(MAXLEN):\n",
        "    predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "    if np.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
        "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  output = tf.squeeze(output, axis=0)\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jB5O9szNrhB"
      },
      "source": [
        "def predict(sentence, model):\n",
        "  prediction = evaluate(sentence, model)\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z26IP46YaFH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014b866c-06b1-40bc-d4fb-7b63528994ce"
      },
      "source": [
        "load_model = Transformer(\n",
        "      vocab_size=12007,\n",
        "      num_layers=NUM_LAYERS,\n",
        "      dff=DFF,\n",
        "      d_model=D_MODEL,\n",
        "      num_heads=NUM_HEADS,\n",
        "      dropout=DROPOUT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 12007, 512)\n",
            "(1, 12007, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9owbR6QVaIJM"
      },
      "source": [
        "load_model.load_weights('/content/drive/MyDrive/ColabNotebooks/project/translation/model/jp2ko_translation_model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DBxT0meg3Fw"
      },
      "source": [
        "def translate(sentences, load_model):\n",
        "  sentence_list = sentences.split(\"。\")\n",
        "  sentence_list.pop()\n",
        "  print(sentence_list)\n",
        "  output_list = []\n",
        "  for sentence in sentence_list:\n",
        "    sentence += \"。\"\n",
        "    output_list.append(predict(sentence, load_model))\n",
        "\n",
        "\n",
        "  outputs = \"\"\n",
        "  for output in output_list:\n",
        "    new_output = []\n",
        "    for word in output:\n",
        "      new_output.append(int(word))\n",
        "    \n",
        "    output_de = ko_tokenizer.decode(new_output)\n",
        "    outputs = outputs + '' + output_de\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu_-aAuVjUBo"
      },
      "source": [
        "sentences = \"英政府は27日、国内でオミクロン株の感染を2件確認したと発表した。「オミクロン株は大変速いスピードで拡大している」。ジョンソン首相は記者会見で危機感をあらわにし、人口の約8割を占めるイングランドの店内や公共交通機関でマスク着用を義務づける新たな行動規制措置を発表した。また、すべての入国者に対して到着後2日以内のPCR検査や陰性が確認されるまでの自主隔離を義務づけるとも表明。オミクロン株の感染者と接触した人については10日間の自主隔離も必要とした。\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4PCikSjh7Ek"
      },
      "source": [
        "output_list = translate(sentences, load_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-j9khIXjLjc"
      },
      "source": [
        "output_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능 확인\n",
        "원문: 英政府は27日、国内でオミクロン株の感染を2件確認したと発表した。「オミクロン株は大変速いスピードで拡大している」。ジョンソン首相は記者会見で危機感をあらわにし、人口の約8割を占めるイングランドの店内や公共交通機関でマスク着用を義務づける新たな行動規制措置を発表した。また、すべての入国者に対して到着後2日以内のPCR検査や陰性が確認されるまでの自主隔離を義務づけるとも表明。オミクロン株の感染者と接触した人については10日間の自主隔離も必要とした。  \n",
        "<br>\n",
        "<br>\n",
        "제작 모델: 영국 정부는 27일 국내에서 오미크론주 감염을 2건 확인했다고 밝혔다.\"오미크론주는 매우 빠른 속도로 확대되고 있다\".존슨 총리는 기자회견에서 위기감을 드러내며 인구 약 8할을 차지하는 잉글랜드점 내나 대중교통에서 마스크 착용을 의무화하는 새로운 행동규제 조치를 발표했다.또 모든 입국자에 대해 도착 후 2일 이내 PCR 검사나 음성이 확인되기까지의 자가격리를 의무화한다고도 표명했다.오미크론주 확진자와 접촉한 사람에 대해서는 10일간의 자가격리도 필요로 했다.  \n",
        "<br>\n",
        "<br>\n",
        "papago: 영국 정부는 27일, 국내에서 오미크론주의 감염을 2건 확인했다고 발표했다.\"오미크론주는 매우 빠른 속도로 확대하고 있다.\" 존슨 수상은 기자 회견에서 위기감을 나타내어, 인구의 약 8할을 차지하는 잉글랜드의 점내나 공공 교통 기관에서 마스크 착용을 의무화 하는 새로운 행동 규제 조치를 발표했다.또, 모든 입국자에 대해서 도착 후 2일 이내의 PCR 검사나 음성이 확인될 때까지의 자율 격리를 의무화한다고도 표명.오미크론주 감염자와 접촉한 사람에 대해서는 10일간의 자율격리도 필요로 했다.  "
      ],
      "metadata": {
        "id": "xqkuQvXlCPkv"
      }
    }
  ]
}
